{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Index Generation\n",
    "Generates indices for train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "from collections import Counter\n",
    "from progressbar import *\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib\n",
    "#from watchmal.testing.repeating_classifier_training_utils import *\n",
    "from functools import reduce\n",
    "\n",
    "# Add the path to the parent directory to augment search for module\n",
    "par_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "if par_dir not in sys.path:\n",
    "    sys.path.append(par_dir)\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from testing.repeating_classifier_training import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_path = \"/data/WatChMaL/data/IWCD_mPMT_Short_e-mu-gamma_E0to1000MeV_digihits.h5\"\n",
    "f = h5py.File(original_data_path, \"r\")\n",
    "\n",
    "labels = np.array(f['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['angles', 'energies', 'event_hits_index', 'event_ids', 'hit_charge', 'hit_pmt', 'hit_time', 'labels', 'positions', 'root_files', 'veto', 'veto2']>\n"
     ]
    }
   ],
   "source": [
    "# Import test events from h5 file\n",
    "original_data_path = \"/data/WatChMaL/data/IWCD_mPMT_Short_e-mu-gamma_E0to1000MeV_digihits.h5\"\n",
    "f = h5py.File(original_data_path, \"r\")\n",
    "\n",
    "print(f.keys())\n",
    "\n",
    "hdf5_hit_pmt = f[\"hit_pmt\"]\n",
    "hdf5_hit_time = f[\"hit_time\"]\n",
    "hdf5_hit_charge = f[\"hit_charge\"]\n",
    "\n",
    "\n",
    "hit_pmt = np.memmap(original_data_path, mode=\"r\", shape=hdf5_hit_pmt.shape,\n",
    "                                    offset=hdf5_hit_pmt.id.get_offset(), dtype=hdf5_hit_pmt.dtype)\n",
    "\n",
    "hit_time = np.memmap(original_data_path, mode=\"r\", shape=hdf5_hit_time.shape,\n",
    "                                    offset=hdf5_hit_time.id.get_offset(), dtype=hdf5_hit_time.dtype)\n",
    "\n",
    "hit_charge = np.memmap(original_data_path, mode=\"r\", shape=hdf5_hit_charge.shape,\n",
    "                                    offset=hdf5_hit_charge.id.get_offset(), dtype=hdf5_hit_charge.dtype)\n",
    "\n",
    "angles     = np.array(f['angles'])\n",
    "energies   = np.array(f['energies'])\n",
    "positions  = np.array(f['positions'])\n",
    "labels     = np.array(f['labels'])\n",
    "root_files = np.array(f['root_files'])\n",
    "\n",
    "#original_radius = [np.sqrt(original_positions[i,0,0]**2 + original_positions[i,0,2]**2) for i in range(original_positions.shape[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up indices\n",
    "indices = np.array(range(len(labels)))\n",
    "# Set up root file set\n",
    "root_file_set = list(set(root_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict set\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Set up dict of file indices\n",
    "file_dict = {}\n",
    "for file in root_file_set:\n",
    "    file_dict[file] = []\n",
    "print(\"Dict set\")\n",
    "\n",
    "for idx, root_file in enumerate(root_files):\n",
    "    file_dict[root_file].append(idx)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files associated with each particle type\n",
    "gamma_indices       = indices[np.where(labels == 0)]\n",
    "gamma_root_file_set = list(set(root_files[gamma_indices]))\n",
    "\n",
    "e_indices        = indices[np.where(labels == 1)]\n",
    "e_root_file_set  = list(set(root_files[e_indices]))\n",
    "\n",
    "mu_indices       = indices[np.where(labels == 2)]\n",
    "mu_root_file_set = list(set(root_files[mu_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define indices retrieval function\n",
    "def get_indices_for_files(file_names):\n",
    "    all_indices = []\n",
    "    for file_name in file_names:\n",
    "        all_indices.extend(file_dict[file_name])\n",
    "    return np.array(all_indices)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17930229 17930230 17930231 ... 18761165 18761166 18761167]\n"
     ]
    }
   ],
   "source": [
    "mu_test_files, mu_val_files, mu_train_files = mu_root_file_set[0:400], mu_root_file_set[400:500], mu_root_file_set[500:]\n",
    "\n",
    "mu_test_set, mu_val_set, mu_train_set = get_indices_for_files(mu_test_files), get_indices_for_files(mu_val_files), get_indices_for_files(mu_train_files)\n",
    "\n",
    "print(mu_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11941526 11941527 11941528 ... 11259914 11259915 11259916]\n"
     ]
    }
   ],
   "source": [
    "gamma_test_files, gamma_val_files, gamma_train_files = gamma_root_file_set[0:400], gamma_root_file_set[400:500], gamma_root_file_set[500:]\n",
    "\n",
    "gamma_test_set, gamma_val_set, gamma_train_set = get_indices_for_files(gamma_test_files), get_indices_for_files(gamma_val_files), get_indices_for_files(gamma_train_files)\n",
    "\n",
    "print(gamma_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6781902 6781903 6781904 ... 6843944 6843945 6843946]\n"
     ]
    }
   ],
   "source": [
    "e_test_files, e_val_files, e_train_files = e_root_file_set[0:400], e_root_file_set[400:500], e_root_file_set[500:]\n",
    "\n",
    "e_test_set, e_val_set, e_train_set = get_indices_for_files(e_test_files), get_indices_for_files(e_val_files), get_indices_for_files(e_train_files)\n",
    "\n",
    "print(e_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1}\n",
      "{0}\n",
      "{2}\n"
     ]
    }
   ],
   "source": [
    "# Verify that indices match\n",
    "all_e_indices = np.concatenate((e_test_set, e_val_set, e_train_set))\n",
    "print(set(labels[all_e_indices]))\n",
    "\n",
    "all_gamma_indices = np.concatenate((gamma_test_set, gamma_val_set, gamma_train_set))\n",
    "print(set(labels[all_gamma_indices]))\n",
    "\n",
    "all_mu_indices = np.concatenate((mu_test_set, mu_val_set, mu_train_set))\n",
    "print(set(labels[all_mu_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20683927\n",
      "20683927\n",
      "20683927\n"
     ]
    }
   ],
   "source": [
    "# Verify that all events are uniquely accounted for\n",
    "all_collected_indices = np.concatenate((e_test_set, e_val_set, e_train_set, gamma_test_set, gamma_val_set, gamma_train_set, mu_test_set, mu_val_set, mu_train_set))\n",
    "\n",
    "print(len(labels))\n",
    "print(len(all_collected_indices))\n",
    "print(len(set(all_collected_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs = np.concatenate((e_train_set, mu_train_set, gamma_train_set))\n",
    "val_idxs   = np.concatenate((e_val_set, mu_val_set, gamma_val_set))\n",
    "test_idxs  = np.concatenate((e_test_set, mu_test_set, gamma_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('representative_indices.npz', train_idxs=train_idxs, val_idxs=val_idxs, test_idxs=test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez('equal_indices.npz', train_idxs=train_idxs, val_idxs=val_idxs, test_idxs=test_idxs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
